---
title: "Spark を使ってみる"
date: 2023-10-23T00:45:39+09:00
tags: [DataEngineering]
draft: true
---

# アーキテクチャ
## ノード
* Driver ... 分散しない処理とExecutorの制御を行う
* Executor ... 分散処理を行うノード

## 処理の構造
* ジョブ ... 複数のステージを内包。処理するファイルの文だけ作られる
* ステージ ... 複数のタスクを内包。単一ノード内で完結するタスクをまとめたもの
* タスク

# データを扱う
## データセット
* RDD ... Spark がデータを扱う際の素のAPI.
* DataFrame ... pandas に似た API が提供されており扱いやすい.


```py
# -----------------------------
# RDD
# SparkContext を通して扱う
# -----------------------------
def proc(val):
    return val * 2

dt_lst = [1,2,3,"aaa"]
dt_rdd = spark.sparkContext.parallelize(dt_lst)
dt_rdd.map(proc).collect()

# -----------------------------
# DataFrame 
# SparkSession を通して扱う。
# このオブジェクトは RDD/Hive/SQL を束ねて扱うための仕組みとして提供されている。
# -----------------------------

# 関数から生成
dt_df = spark.range(0, 5)

# 配列から生成
dt_df = spark.createDataFrame([[1,2,3]], ["col1", "col2", "col3"])

# フィールドの情報を取得
print(df.schema)
df.printSchema()

# 取得した DataFrame の内容を表示
dt_df.display()
display(dt_df)
```

## ロード & セーブ
SparkSQL での書き方

```sql
-- ファイパスを指定してロードする
SELECT * FROM csv.`/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv` LIMIT 10

-- ファイルパスを指定して永続テーブル化する (SQL)
DROP TABLE IF EXISTS diamonds;
CREATE TABLE diamonds
USING CSV OPTIONS (path "/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv", header "true");

-- ファイルパスを指定して一時テーブル化する (SQL)
DROP TABLE IF EXISTS diamonds_tmp;
CREATE TEMP TABLE diamonds_tmp
USING CSV OPTIONS (path "/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv", header "true");

-- 指定したテーブルへデータロード
COPY INTO diamonds
FROM '/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv' FILEFORMAT = CSV
FORMAT_OPTIONS ('inferSchema'='true', 'header'='true')
```

Python での書き方

```py
from pyspark.sql.types import *

# テーブルから生成
dt_df = spark.table("samples.tpch.customer")

# ファイルパスを指定して読み込む (Python)
df = spark.read \
    .options(header=True, inferSchema=True) \
    .csv("/databricks-datasets/retail-org/customers/")

df = spark.read \
    .format("csv") \
    .options(header=True, inferSchema=True) \
    .load("/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv") \
    .limit(10)

# 一時テーブルを生成
df.createOrReplaceTempView("diamonds_tmp")
# 永続テーブルを生成
df.write.saveAsTable("diamonds")

# 構造化ストリーミングで読み込み
# ストリーミングで読み込む場合にはスキーマ (後述) の指定が必要
df = spark.readStream \
    .format("csv") \
    .options(header=True) \
    .schema(
        StructType([
            StructField('customer_id', IntegerType(), True),
            StructField('tax_id', DoubleType(), True),
            StructField('tax_code', StringType(), True),
            StructField('customer_name', StringType(), True),
            StructField('state', StringType(), True),
            StructField('city', StringType(), True),
            StructField('postcode', StringType(), True),
            StructField('street', StringType(), True),
            StructField('number', StringType(), True),
            StructField('unit', StringType(), True),
            StructField('region', StringType(), True),
            StructField('district', StringType(), True),
            StructField('lon', DoubleType(), True),
            StructField('lat', DoubleType(), True),
            StructField('ship_to_address', StringType(), True),
            StructField('valid_from', IntegerType(), True),
            StructField('valid_to', DoubleType(), True),
            StructField('units_purchased', DoubleType(), True),
            StructField('loyalty_segment', IntegerType(), True)
        ])
    ) \
    .load("/databricks-datasets/retail-org/customers/")
display(df)
```

## 読み込み時オプション
ファイルを読み込む際にオプションを指定可能。

```py
# 参考: https://spark.apache.org/docs/latest/sql-data-sources-csv.html
spark.read \
    .option("inferSchema", True) \
    .options(inferSchema=True) \
    .csv(path, inferSchema=True)
```

| Prop        | Desc                                                                                                      | Default |
| ----------- | --------------------------------------------------------------------------------------------------------- | ------- |
| sep         | 区切り文字.                                                                                               | カンマ  |
| encoding    | 文字コード                                                                                                | UTF-8   |
| header      | 先頭行から項目名を取得するか                                                                              | False   |
| inferSchema | 1度読み込んでスキーマを推測した後に改めて読み込む. 推測しない場合は全て文字列型になる. date型は未サポート | False   |

## 書き込み時オプション
ファイルを書き込む際にオプションを指定可能。

オプションの compression は parquet で書き出す際に使用できるオプション。
https://spark.apache.org/docs/latest/sql-data-sources-parquet.html

```py
spark.write \
    .option("compression", "snappy") \
    .mode("append|overwrite|error|ignore") \
    .parquet(path)
```

## スキーマ
DataFrame は項目定義の情報としてスキーマを要する。  
これはソースデータから推定する形で生成されるが、明示も可能。ストリーミングを使う際など推定が出来ない場合では明示することになる。

```py
from pyspark.sql.types import *

# 配列から生成 (スキーマ明示)
dt_df = spark.createDataFrame(
    [
        (1, {"prop1": 111, "prop2": "hoge1"}),
        (2, {"prop1": 222, "prop2": "hoge2"}),
        (3, {"prop1": 333, "prop2": "hoge3"}),
    ],
    StructType([
        StructField("RecordID", IntegerType(), False),
        StructField(
            "Info",
            StructType([
                StructField("prop1", IntegerType(), True),
                StructField("prop2", StringType(), True)
            ])
        )
    ])
)
```

## クエリ作成
SQL で使用するワードを反映したメソッドを呼び出すことで集計できる。
複雑な条件を定義したい場合は項目に対し `pyspark.sql.Column` クラスのインスタンスを生成し、クラスのメソッドを使うことで定義できる。

```py
from pyspark.sql.functions import col, when, otherwise

spark.table("products") \
    .where(col("price") < 200) \
    .orderBy(col("price").desc(), "name") \
    .select(
        "name",
        col("price").cast("string"),
        when(col("price") > 10, 1).otherwise(2)
    ) \
    .display()

# 項目の取得には幾つかの方法がある。
df.columnName
df["columnName"]
col("columnName")
```

## 値操作
Column クラスを用いる事で定義できる集計式は以下の通り。以下に幾つか列挙する。  
https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/column.html

| Method                   | Description                  |
| ------------------------ | ---------------------------- |
| \*, + , <, >=, ==, !=    | 等価・比較演算子             |
| alias                    | 列にエイリアスを与えます     |
| cast, astype             | 列を異なるデータ型にキャスト |
| isNull, isNotNull, isNan | null、非null、NaNの判断      |
| asc, desc                | 列の昇順/降順                |

Column クラスの生成は pyspark.sql.functions でメソッドが多く提供されている。  
CASE 文などもこれを用いて表現する。  
https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html

```py
from pyspark.sql.functions import col, when

df.withColumn("hoge",
    when(col("foo") < 100, "value1").
    when(col("foo") < 200, "value2").
    otherwise("value3")
)
```

| Method | Description                                                    |
| ------ | -------------------------------------------------------------- |
| ceil   | 指定された列を正の値の場合は切上げ、負の値の場合は切り下げする |
| cos    | 指定された値の余弦を計算する                                   |
| log    | 指定された値の自然対数を計算する                               |
| round  | HALF_UP丸めモードで0桁までの列eの値を返する                    |
| sqrt   | 指定された浮動小数点値の平方根を計算する                       |

グループ化したい場合は以下のようにする。

```py
import pyspark.sql.functions as F

# グループ化
df.groupBy("columnName")
df.groupBy("columnName1", "columnName2")
df.groupBy(["columnName1", "columnName2"])

# グループ集計
df.groupBy("columnName").sum("sumColumn1", "sumColumn2")
df.groupBy("columnName").agg(
    F.sum("sumColumn1").alias("sumColumn1"),
    F.avg("avgColumn2").alias("avgColumn2")
)
```

日付の扱いも pyspark.sql.functions に便利なメソッドが提供されている。

```py
df = spark \
    .createDataFrame([("2023-11-05 21:00:00",)], ["date_text"]) \
    .select(
        F.to_utc_timestamp(F.to_timestamp("date_text", "yyyy-MM-dd HH:mm:ss"), "Asia/Tokyo").alias("date_dt"),
        F.dayofweek(F.current_timestamp()),
        F.unix_timestamp(F.col("date_text"), "yyyy-MM-dd HH:mm:ss").alias("unixtime")
    ) \
    .display()
```

| Method                                        | Description                                     |
| --------------------------------------------- | ----------------------------------------------- |
| add_months(col: Column, months: int): Column  | months ヶ月足す                                 |
| current_timestamp(): Column                   | 現在の日時                                      |
| date_format(col: Column, format: str): Column | 日付/タイムスタンプ/文字列を文字に変換          |
| dayofweek(col: Column)                        | 日曜～月曜を1～7の数値に変換                    |
| from_unixtime(col: Column, format: str)       | Unixタイムを format 形式の文字列へ変換          |
| minute(col: Column)                           | 分を数値として抽出                              |
| unix_timestamp(col: Column, format: str)      | format 形式で表現された日付を Unix タイムに変換 |

pyspark.sql.functions には他にも便利な関数がある。

| Method                | Description                          |
| --------------------- | ------------------------------------ |
| approx_count_distinct | 一意のアイテムの近似数を返す         |
| avg                   | 平均値を返す                         |
| collect_list          | 重複を含むオブジェクトのリストを返す |
| corr                  | 2つの列のピアソン相関係数を返す      |
| max                   | 最大値を計算する                     |
| mean                  | 平均値を計算する                     |
| stddev_samp           | サンプル標準偏差を返す               |
| sumDistinct           | 式内の一意の値の合計を返す           |
| var_pop               | 母集団分散を返す                     |
| split                 | 項目の値を引数の値で分割する         |

## 表操作
項目を新たに作ったり、名前を変更する場合は以下の通り。

```py
# 項目作成
dt_df.withColumn("columnName", col: Column)
dt_df.withColumnRename("oldColumnName", "newColumnName")

# 項目削除
dt_df.drop("columnName")

# データフレーム統合
# union all 相当. 統合対象のスキーマは順序レベルで同一にする必要があるが、ByName の場合は項目名で突合される
dt_df.union(df_df2)
dt_df.unionByName(df_df2)
```

欠落データに対するメソッドをまとめたものとして [pyspark.sql.DataFrameNaFunctions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameNaFunctions.html) もある。  

```py
# null 行の削除
dt_df.na.drop(
    how: str = 'any(default)|all',
    thresh: Optional[int] = None,
    subset: Union[str, List[str], None] = None
)
# null 列へ代替値を代入
dt_df.na.fill(value, subset: Optional[List[str]])
# 任意の値へ代替値を代入. dt_df.replace のエイリアス
dt_df.na.replace(newvalue, oldvalue, subset: Optional[List[str]])
```

テーブル結合の操作は以下の通り。

```py
joined_df = dt_df.join(other=dt_df2, on='columnName', how = "inner")
```

ネストデータの扱いは以下の通り。

```py
import pyspark.sql.types as T
import pyspark.sql.functions as F

# リストオブジェクトを定義したデータフレームを定義
dt_df = spark.createDataFrame(
    [
        (1, [{"prop1": 111, "prop2": "hoge1"}, {"prop1": 222, "prop2": "hoge2"}]),
        (2, [{"prop1": 444, "prop2": "hoge1"}, {"prop1": 555, "prop2": "hoge2"}]),
        (3, [{"prop1": 777, "prop2": "hoge1"}, {"prop1": 888, "prop2": "hoge2"}]),
    ],
    T.StructType([
        T.StructField("RecordID", T.IntegerType(), False),
        T.StructField(
            "Info",
            T.ArrayType(
                T.StructType([
                    T.StructField("prop1", T.IntegerType(), True),
                    T.StructField("prop2", T.StringType(), True)
                ])
            )
        )
    ])
)
dt_df.display()

# リストオブジェクトを展開
dt_df = dt_df.withColumn("Info", F.explode("Info"))
dt_df.display()

# 行展開をリストオブジェクトへ戻す
dt_df = dt_df.groupBy("RecordID").agg(F.collect_list("Info"))
dt_df.display()
```

| Method                    | Description                                               |
| ------------------------- | --------------------------------------------------------- |
| explode(col: Column)      | リストオブジェクトを行方向へ展開                          |
| collect_list(col: Column) | グループ化された項目をリストオブジェクトへ変換 (重複有り) |
| collect_set(col: Column)  | グループ化された項目をリストオブジェクトへ変換 (重複無し) |


## 遅延評価
Spark では集計処理を書いただけでは処理は行われない。遅延評価を実行するメソッドを呼び出すことで処理が実行される。

| Method                     | Description                                |
| -------------------------- | ------------------------------------------ |
| collect, first, head, take | 全/最初の1/最初のn行を返す                 |
| count                      | 行数を返す                                 |
| show                       | 上位n行を表形式で表示                      |
| describe, summary          | 数値および文字列の列に対する基本統計を計算 |

## キャッシュ

* DataFrame.cache
* DataFrame.unpersist

# SparkSQL との相互運用

```py
# DataFrame を SparkSQL から生成
dt_df = spark.sql("SELECT * FROM samples.tpch.customer")

# DataFrame を SparkSQL から参照できるよう登録する
dt_df.createOrReplaceTempView("hoge")

# SparkSQL で登場する式を使用して各値を取り出せる
dt_df.selectExpr("expr1", "expr2" ...)
```
